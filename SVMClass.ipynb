{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code for Google Colab\n",
    "USE_COLAB = False\n",
    "GOOGLE_DRIVE_PATH = ''\n",
    "if USE_COLAB:\n",
    "    print(\"Using Colab!\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    import os\n",
    "    GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'EECS545/SVM-Variations'\n",
    "    GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "    print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "\n",
    "    import sys\n",
    "    sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torchvision\n",
    "    import statistics\n",
    "    import random\n",
    "    import time\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    import copy\n",
    "    import shutil\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    !pip3 install -q idx2numpy\n",
    "\n",
    "    if torch.cuda.is_available:\n",
    "      print('Good to go!')\n",
    "    else:\n",
    "      print('Please set GPU via Edit -> Notebook Settings.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import SVMClass\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "############\n",
    "# load and plot data\n",
    "############\n",
    "if USE_COLAB:\n",
    "    nuclear = sio.loadmat(os.path.join(GOOGLE_DRIVE_PATH,'Data/nuclear.mat'))\n",
    "else:\n",
    "    nuclear = sio.loadmat('Data/nuclear.mat')\n",
    "\n",
    "x = torch.tensor(nuclear['x'], dtype = torch.float32, device = 'cuda')\n",
    "y = torch.tensor(nuclear['y'], dtype = torch.long, device = 'cuda')\n",
    "\n",
    "N = x.shape[1]\n",
    "\n",
    "## reshape data\n",
    "x = x.t()               # x \\in (N,D)\n",
    "y = y.reshape(N)        # y \\in (N,)\n",
    "\n",
    "# plot the data:\n",
    "negInd = y == -1\n",
    "posInd = y == 1\n",
    "plt.scatter(x.cpu()[negInd, 0], x.cpu()[negInd, 1], color='b')\n",
    "plt.scatter(x.cpu()[posInd, 0], x.cpu()[posInd, 1], color='r')\n",
    "plt.figure(1)\n",
    "plt.show()\n",
    "\n",
    "# change the label into {0,1}\n",
    "# y = (y+1)//2\n",
    "\n",
    "# Preprocess: append 1s at the end of data vectors\n",
    "ones_x = torch.ones((N,1), dtype = torch.float32, device = 'cuda')\n",
    "x_with_ones = torch.cat((x,ones_x),axis=1)\n",
    "x = x_with_ones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-9014a466bc9d>, line 16)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-9014a466bc9d>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    self.W, _ ==\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# General SVM Class that will be a base for all the other SVM Implementations\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class SVM(object):\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "        np.random.seed(0)\n",
    "        torch.manual_seed(0)\n",
    "        self.default_lr = 0.5\n",
    "        self.default_reg = 1e-3\n",
    "  \n",
    "    def train(self, X_train, y_train, learning_rate=0.0, reg=0.0, num_iters=100,\n",
    "            batch_size=200, print_progress=False):\n",
    "        \"\"\"\n",
    "            Takes in the training data and labels as well as training parameters.\n",
    "            Updates the weights using stochastic gradient descent\n",
    "\n",
    "            Inputs:\n",
    "            - X_train: A PyTorch tensor of shape (N, D) containing training data; there are N\n",
    "            training samples each of dimension D.\n",
    "            - y_train: A PyTorch tensor of shape (N,) containing training labels; y[i] = {-1,1}\n",
    "            means that X[i] has label  -1 or 1 depending on the class.\n",
    "            - learning_rate: (float) learning rate for optimization.\n",
    "            - reg: (float) regularization strength. (ie. lambda)\n",
    "            - num_iters: (integer) number of steps to take when optimizing\n",
    "            - batch_size: (integer) number of training examples to use at each step.\n",
    "            - print_progress: (boolean) If true, print progress during optimization.\n",
    "            - exit_diff: (float) condition to stop the gradient descent algorithm if the\n",
    "            change in loss is too low.\n",
    "\n",
    "            Returns: A tuple of:\n",
    "            - loss_all: A PyTorch tensor giving the values of the loss at each\n",
    "                training iteration.\n",
    "        \"\"\"\n",
    "        if learning_rate == 0: # if values not assigned to it, assigned default values\n",
    "            learning_rate=self.default_lr \n",
    "            reg=self.default_reg\n",
    "        \n",
    "        self.W, loss_history = train_linear_classifier(self.loss, self.W, X_train, y_train, learning_rate, reg, num_iters, batch_size, print_progress)\n",
    "        \n",
    "        return loss_history\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Takes in the test data and outputs a prediction torch\n",
    "        Inputs: \n",
    "        - X: A PyTorch tensor of shape (N, D) containing N data points and each point has dimension D.\n",
    "        - self.W: A PyTorch tensor of shape (D, C) containing the weights\n",
    "        Output:\n",
    "        - y_pred: A PyTorch tensor of shape (N,) containing 0~(C-1) labels for the X\n",
    "        \"\"\"\n",
    "        prob = torch.matmul(X,self.W) # (N, C)\n",
    "        \n",
    "        # y_pred = torch.argmax(prob, dim=1)\n",
    "        y_pred = torch.sign(prob).reshape(-1)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def loss(self, W, X_batch, y_batch, reg = 0.0):\n",
    "        # function that we will override in the children classes\n",
    "        \n",
    "        if reg == 0: # if values not assigned to it, assigned default values\n",
    "            reg=self.default_reg\n",
    "        pass\n",
    "    \n",
    "class LinearSVM(SVM):\n",
    "    def loss(self, W, X_batch, y_batch, reg = 0.0):\n",
    "        \"\"\"\n",
    "        Compute the loss function and its derivative.\n",
    "        Subclasses will override this.\n",
    "\n",
    "        Inputs:\n",
    "        - W: A PyTorch tensor of shape (D, C) containing (trained) weight of a model.\n",
    "        - X_batch: A PyTorch tensor of shape (N, D) containing a minibatch of N\n",
    "          data points; each point has dimension D.\n",
    "        - y_batch: A PyTorch tensor of shape (N,) containing labels for the minibatch.\n",
    "        - reg: (float) regularization strength.\n",
    "\n",
    "        Returns: A tuple containing:\n",
    "        - loss as a single float\n",
    "        - dW: gradient with respect to self.W; an tensor of the same shape as W\n",
    "        \"\"\"\n",
    "        if reg == 0: # if values not assigned to it, assigned default values\n",
    "            reg=self.default_reg\n",
    "        \n",
    "        # Let autograd get us the gradient with respect to W\n",
    "        W.requires_grad = True \n",
    "        \n",
    "        # Wei-Chun's method: 2-class SVM with y = {-1,1} converted to use torch functions\n",
    "        # This assumes W is Dx1 \n",
    "        N = X_batch.shape[0]\n",
    "        y_pred = X_batch.matmul(W) # N-sized array\n",
    "        \n",
    "        # get svm loss term\n",
    "        loss_array = 1.0 - y_batch * y_pred \n",
    "        \n",
    "        # apply the max function when compared to 0\n",
    "        loss_array[loss_array < 0.0] = 0.0 \n",
    "        \n",
    "        # get the overall loss mean\n",
    "        loss = torch.mean(loss_array) \n",
    "        \n",
    "        # add regularization term\n",
    "        loss += 0.5 * reg * torch.sum(W * W) \n",
    "        # print(loss)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dw = W.grad.clone()\n",
    "            W.grad.zero_()\n",
    "        # stop the gradient calculations since we are done\n",
    "        W.requires_grad = False\n",
    "\n",
    "                \n",
    "        '''\n",
    "        # calculate the multiclass SVM loss\n",
    "        N = X_batch.shape[0]\n",
    "        scores = (X_batch.matmul(W)).t() # CxN matrix\n",
    "        temp_index = torch.arange(N)\n",
    "        correct_scores = scores[y_batch, temp_index]\n",
    "        Margins = scores - correct_scores + torch.ones_like(scores, device = X_batch.device, dtype = X_batch.dtype)\n",
    "        Margins[Margins<0] = 0 # Apply the max function\n",
    "\n",
    "        loss = Margins.sum()/N - 1 # get the average loss and subtract 1 for the j=y_i case\n",
    "        loss += 0.5 * reg * torch.sum(W * W)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        return loss, dw\n",
    "\n",
    "\n",
    "# Training function for svm classifier. Applies SGD.\n",
    "def train_linear_classifier(loss_fun, W, X, y, learning_rate=0.5, reg=1e-3, num_iters=1000,\n",
    "        batch_size=200, print_progress=False):\n",
    "    '''\n",
    "    Output:\n",
    "    - W:            A PyTorch tensor giving the weight of SVM predictor\n",
    "    - loss_history: A PyTorch tensor giving the values of the loss at each training iteration.\n",
    "    '''\n",
    "    N, D = X.shape\n",
    "\n",
    "    # initialize weight\n",
    "    if W is None:\n",
    "        # C = 1\n",
    "        # W = torch.zeros([D, C], dtype=X.dtype, device=X.device)\n",
    "        W = 1 * torch.randn(D, device=X.device, dtype=X.dtype)\n",
    "\n",
    "\n",
    "    # Stochastic Gradient Descent\n",
    "    loss_history = torch.zeros(num_iters, dtype=X.dtype, device=X.device)\n",
    "    for it in range(num_iters):\n",
    "        # sample batch\n",
    "        X_batch, y_batch = sample_batch(X, y, N, batch_size)\n",
    "\n",
    "        # compute loss and gradient\n",
    "        loss, grad = loss_fun(W, X_batch, y_batch, reg)\n",
    "        loss_history[it] = loss\n",
    "\n",
    "        # update weight\n",
    "        W -= learning_rate*grad\n",
    "\n",
    "        if print_progress and it % 100 == 0:\n",
    "            print('iteration %d / %d: loss %f' % (it, num_iters, loss_history[it]))\n",
    "\n",
    "    # return \n",
    "    return W, loss_history\n",
    "\n",
    "\n",
    "\n",
    "# Sample batch of data\n",
    "def sample_batch(X, y, N, batch_size):\n",
    "    # randomly sample \"batch_size\" (Xi,yi) in (X,y)\n",
    "\n",
    "    inx = torch.randint(0, N, [batch_size])\n",
    "\n",
    "    X_batch = X[inx, :]\n",
    "    y_batch = y[inx]\n",
    "\n",
    "    return X_batch, y_batch\n",
    "    \n",
    "    \n",
    "# search for hyperparameters:\n",
    "def search_for_hyperparams(model, x, y, folds, learning_rates, regs):\n",
    "    \"\"\"\n",
    "        model to evaluate\n",
    "        x: full training data NxD\n",
    "        y: full training data labels N\n",
    "        folds: number of folds for k fold cross validation\n",
    "        learning_rates: list of learning rates to test\n",
    "        regs: list of regularization terms to test (lambda)\n",
    "        \n",
    "        outputs: max_accuracy, max_learning_rate, max_reg\n",
    "    \"\"\"\n",
    "    # divide data into training and validation sets:\n",
    "    N = x.shape[0]\n",
    "\n",
    "    # get random indices for dataset folds\n",
    "    indices = torch.randperm(N)\n",
    "    length = N // folds\n",
    "\n",
    "    max_acc = 0.0\n",
    "    max_reg = 0.0\n",
    "    max_lr = 0.0\n",
    "    for reg in regs:\n",
    "        for lr in learning_rates:\n",
    "            acc = torch.zeros(folds)\n",
    "            for k in range(folds):\n",
    "                x_train = torch.cat((x[indices[0:k*length]],x[indices[(k+1)*length:]]),dim = 0)\n",
    "                y_train = torch.cat((y[indices[0:k*length]],y[indices[(k+1)*length:]]),dim = 0)\n",
    "                x_val = x[indices[k*length:(k+1)*length]]\n",
    "                y_val = y[indices[k*length:(k+1)*length]]\n",
    "\n",
    "                model.__init__()\n",
    "                model.train(x_train, y_train, reg=reg, num_iters=1000, learning_rate=lr, print_progress = False)\n",
    "                y_pred = model.predict(x_val)\n",
    "                acc[k] = ((y_val==y_pred).sum()) / float(y_val.shape[0])\n",
    "            acc_mean = acc.mean()\n",
    "            if(acc_mean > max_acc):\n",
    "                max_acc = acc_mean\n",
    "                max_reg = reg\n",
    "                max_lr = lr\n",
    "\n",
    "            print(\"at reg = {} and lr = {} we get acc = {}\" .format(reg, lr, acc_mean))\n",
    "\n",
    "\n",
    "    print(\"Max we get at: reg: {} and lr = {} we get acc = {}\" .format(max_reg, max_lr, max_acc))\n",
    "    return max_acc, max_lr, max_reg\n",
    "    \n",
    "\n",
    "# function that uses library svm on input dataset and outputs accuracy\n",
    "def apply_sklearn_svm(x_train, y_train, x_test = [], y_test = [], max_iters = 1000):\n",
    "    # Linear SVM from library:\n",
    "    \n",
    "    if len(x_test) == 0:\n",
    "        x_test = x_train\n",
    "        y_test = y_train\n",
    "    \n",
    "    # Linear SVM on this dataset\n",
    "    x_train = x_train.cpu()\n",
    "    y_train = y_train.cpu()\n",
    "    x_test = x_test.cpu()\n",
    "    y_test = y_test.cpu()\n",
    "    \n",
    "    from sklearn.svm import SVC, LinearSVC\n",
    "    svclassifier = LinearSVC(max_iter = max_iters)\n",
    "    svclassifier.fit(x_train, y_train)\n",
    "    y_pred = torch.tensor(svclassifier.predict(x_test))\n",
    "    acc = ((y_test==y_pred).sum()) / float(y_test.shape[0])\n",
    "    print(acc)\n",
    "    \n",
    "    \n",
    "    # Sklearn for kernel svm:\n",
    "    \"\"\"\n",
    "    from sklearn.svm import SVC\n",
    "    svclassifier = SVC(kernel='linear')\n",
    "    svclassifier.fit(x_train, y_train)\n",
    "    y_pred = torch.tensor(svclassifier.predict(x_test))\n",
    "    acc = ((y_test==y_pred).sum()) / float(y_test.shape[0])\n",
    "    print(acc)\n",
    "    \"\"\"\n",
    "    \n",
    "# function that applies our implemented LSVM with cross validation\n",
    "def apply_LSVM(x_train, y_train, x_test = [], y_test = [], max_iters = 1000, cross = False):\n",
    "    # get best learning rate and evaluate accuracy on test data\n",
    "    # cross indicates whether we want to search for hyperparameters or just use the default ones\n",
    "    \n",
    "    if len(x_test) == 0:\n",
    "        x_test = x_train\n",
    "        y_test = y_train\n",
    "        \n",
    "    LSVM = LinearSVM()\n",
    "    \n",
    "    # apply cross validation if cross:\n",
    "    if cross:\n",
    "        learning_rates = [1e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1]\n",
    "        regs = [1e-4, 1e-3, 5e-3, 1e-2, 1e-1]\n",
    "        folds = 10\n",
    "\n",
    "        max_acc, max_lr, max_reg = search_for_hyperparams(LSVM, x_train, y_train, folds, learning_rates, regs)\n",
    "        LSVM = LinearSVM()\n",
    "        loss_history = LSVM.train(x_train, y_train, reg=max_reg, num_iters=max_iters, learning_rate=max_lr, print_progress = False)\n",
    "    else:\n",
    "        loss_history = LSVM.train(x_train, y_train, num_iters=max_iters, print_progress = False)\n",
    "        \n",
    "    y_pred = LSVM.predict(x_test)\n",
    "    acc = ((y_test==y_pred).sum())/float(y_test.shape[0])\n",
    "    print(acc)\n",
    "    return loss_history, LSVM\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying with EECS545 HW3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# get accuracy of sk lib svm implementation and our accuracy\n",
    "#########################\n",
    "from LinearSVM import *\n",
    "\n",
    "N = x.shape[0]\n",
    "indices = torch.randperm(N)\n",
    "train_N = N # 3 quarters of data is training. \n",
    "x_train = x[indices[0:train_N]]\n",
    "y_train = y[indices[0:train_N]]\n",
    "x_test = x[indices[train_N:]]\n",
    "y_test = y[indices[train_N:]]\n",
    "\n",
    "apply_sklearn_svm(x_train, y_train, x_test, y_test, max_iters = 1000)\n",
    "\n",
    "loss_history, LSVM = apply_LSVM(x_train, y_train, x_test, y_test, max_iters = 1000)\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "# plot our classifier line and Loss\n",
    "#############################\n",
    "W = LSVM.W[:]\n",
    "\n",
    "w1 = W[0].cpu()\n",
    "w2 = W[1].cpu()\n",
    "b = W[2].cpu()\n",
    "\n",
    "x0_vals = torch.tensor(np.linspace(0, 8, 8)).reshape(1,-1)\n",
    "y_vals = (-b - w1*x0_vals) / w2\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(x0_vals[0,:], y_vals[0,:], color='black', label='Learned line')\n",
    "plt.title('data')\n",
    "# plt.show()\n",
    "\n",
    "negInd = y == -1\n",
    "posInd = y == 1\n",
    "plt.scatter(x.cpu()[negInd, 0], x.cpu()[negInd, 1], color='b')\n",
    "plt.scatter(x.cpu()[posInd, 0], x.cpu()[posInd, 1], color='r')\n",
    "plt.figure(1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(loss_history.cpu().detach().numpy())\n",
    "plt.title('Objective function J')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for hyperparameters that yield max accuracy\n",
    "model = LinearSVM()\n",
    "learning_rates = [1e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1, 2]\n",
    "regs = [1e-4, 1e-3, 5e-3, 1e-2, 1e-1]\n",
    "folds = 10\n",
    "\n",
    "search_for_hyperparams(model, x_train, y_train, folds, learning_rates, regs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mnist Test Code\n",
    "from LinearSVM import *\n",
    "import Mnist_loader as mnist\n",
    "x_train, y_train, x_test, y_test = mnist.load_odd_even_Mnist()\n",
    "x_train, x_test, mu, std = mnist.preprocess_Mnist(x_train, x_test)\n",
    "apply_LSVM(x_train, y_train, x_test, y_test, max_iters = 1000)\n",
    "apply_sklearn_svm(x_train, y_train, x_test, y_test, max_iters = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMGuide1 Test Code\n",
    "USE_COLAB = False\n",
    "GOOGLE_DRIVE_PATH = ''\n",
    "from LinearSVM import *\n",
    "from SVMGuide_loader import load_SVMGuide1\n",
    "x_train, y_train, x_test, y_test, mu, std = load_SVMGuide1(USE_COLAB, GOOGLE_DRIVE_PATH)\n",
    "apply_LSVM(x_train, y_train, x_test, y_test, max_iters = 1000)\n",
    "apply_sklearn_svm(x_train, y_train, x_test, y_test, max_iters = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to compare different models in time and accuracy:\n",
    "from LinearSVM import *\n",
    "from Evaluate_models import *\n",
    "    \n",
    "\n",
    "# define models:\n",
    "LSVM = LinearSVM()  \n",
    "# KSVM = KernelSVM()\n",
    "# CSVM = CSVM()\n",
    "\n",
    "# evaluate using Mnist:\n",
    "LSVM_time_oe, LSVM_acc_oe, LSVM_time_38, LSVM_acc_38 = evaluate_using_mnist(LSVM)\n",
    "# KSVM_time_oe, KSVM_acc_oe, KSVM_time_38, KSVM_acc_38 = evaluate_using_mnist(KSVM)\n",
    "# CSVM_time_oe, CSVM_acc_oe, CSVM_time_38, CSVM_acc_38 = evaluate_using_mnist(CSVM)\n",
    "\n",
    "# evaluate using SVM_Guide1:\n",
    "LSVM_time_svmg, LSVM_acc_svmg = evaluate_using_SVM_Guide1(LSVM)\n",
    "# KSVM_time_svmg, KSVM_acc_svmg = evaluate_using_SVM_Guide1(KSVM)\n",
    "# CSVM_time_svmg, CSVM_acc_svmg = evaluate_using_SVM_Guide1(CSVM)\n",
    "\n",
    "\n",
    "print(\"With Mnist odd/even dataset, we get:\")\n",
    "print(\"Using LSVM: Time = \", LSVM_time_oe, \"  acc = \", LSVM_acc_oe)\n",
    "# print(\"Using KSVM: Time = \", KSVM_time_oe, \"  acc = \", KSVM_acc_oe)\n",
    "# print(\"Using CSVM: Time = \", CSVM_time_oe, \"  acc = \", CSVM_acc_oe)\n",
    "\n",
    "print(\"With Mnist 3/8 dataset, we get:\")\n",
    "print(\"Using LSVM: Time = \", LSVM_time_38, \"  acc = \", LSVM_acc_38)\n",
    "# print(\"Using KSVM: Time = \", KSVM_time_38, \"  acc = \", KSVM_acc_38)\n",
    "# print(\"Using CSVM: Time = \", CSVM_time_38, \"  acc = \", CSVM_acc_38)\n",
    "\n",
    "print(\"With SVM_Guide1 dataset, we get:\")\n",
    "print(\"Using LSVM: Time = \", LSVM_time_svmg, \"  acc = \", LSVM_acc_svmg)\n",
    "# print(\"Using KSVM: Time = \", KSVM_time_svmg, \"  acc = \", KSVM_acc_svmg)\n",
    "# print(\"Using CSVM: Time = \", CSVM_time_svmg, \"  acc = \", CSVM_acc_svmg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
