{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# EECS545 Hw3 (b)\n",
    "# Subgradient.py\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "nuclear = sio.loadmat('nuclear.mat')\n",
    "\n",
    "x = nuclear['x']\n",
    "y = nuclear['y']\n",
    "\n",
    "ones_x = np.ones((1,x.shape[1]))\n",
    "x_with_ones = np.concatenate((ones_x,x))\n",
    "\n",
    "n = x.shape[1]\n",
    "reg = 0.001/(n) # lambda/n \n",
    "\n",
    "def get_L(theta, x_i, y_i):\n",
    "    z = 1 - y_i * x_i.T.dot(theta)\n",
    "    if(z>0):\n",
    "        return z\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def get_J_i(theta, x_i, y_i):\n",
    "    b = theta[0]**2\n",
    "    L = get_L(theta, x_i, y_i)\n",
    "    norm_theta_2 = theta.T.dot(theta)\n",
    "    term2 = reg * (norm_theta_2 - b) / 2\n",
    "    term1 = L/n\n",
    "    J_i = term1 + term2\n",
    "    J_i = np.reshape(J_i, (1,))\n",
    "    return J_i\n",
    "\n",
    "def get_u_i(theta, x_i, y_i):\n",
    "    term2 = reg * (theta - theta[0])\n",
    "    term1 = -(y_i/n) * x_i\n",
    "    u_i = term2\n",
    "    \n",
    "    u_i = np.reshape(term1 + term2, (3,))\n",
    "    #L = get_L(theta, x_i, y_i)\n",
    "    #if(L>0):\n",
    "    #    u_i += term1\n",
    "    #u_i = np.reshape(u_i, (3,))\n",
    "    return u_i\n",
    "\n",
    "\n",
    "# main code:\n",
    "# initialize theta\n",
    "theta = np.zeros((x_with_ones.shape[0],1))\n",
    "max_iter = 100\n",
    "sum_J_i = 0\n",
    "exit_diff = 0.0005\n",
    "J = np.zeros((max_iter,))\n",
    "num_iters = np.zeros((max_iter,))\n",
    "\n",
    "for j in range(1,max_iter):\n",
    "    u = np.zeros((theta.shape[0],n))\n",
    "    # J = np.zeros((1,n))\n",
    "    prev_J = sum_J_i\n",
    "    sum_J_i = 0\n",
    "    for i in range(n):\n",
    "        x_i = np.reshape(x_with_ones[:,i],(3,1))\n",
    "        y_i = y[:,i]\n",
    "\n",
    "        # J[:,i] = get_J_i(theta, x_i, y_i)\n",
    "        sum_J_i += get_J_i(theta, x_i, y_i)\n",
    "        u[:,i] = get_u_i(theta, x_i, y_i)\n",
    "    sum_u_i = np.sum(u,axis=1)\n",
    "    # sum_J_i = np.sum(J,axis=1) \n",
    "    J[j-1] = sum_J_i\n",
    "    num_iters[j-1] = j\n",
    "    \n",
    "    # stopping condition\n",
    "    diff = np.abs(prev_J - sum_J_i)\n",
    "    if(diff < exit_diff):\n",
    "        break\n",
    "    step = 100/j\n",
    "    theta -= step * np.reshape(sum_u_i, (3,1))\n",
    "\n",
    "print(\"theta is: \", theta)\n",
    "print(\"last objective value is: \", J[j-1])\n",
    "\n",
    "\n",
    "# first plot code:\n",
    "negInd = y == -1\n",
    "posInd = y == 1\n",
    "plt.scatter(x[0, negInd[0, :]], x[1, negInd[0, :]], color='b')\n",
    "plt.scatter(x[0, posInd[0, :]], x[1, posInd[0, :]], color='r')\n",
    "\n",
    "# code to plot the separating line:\n",
    "b = theta[0]\n",
    "w1 = theta[1]\n",
    "w2 = theta[2]\n",
    "x1 = x[0,:]\n",
    "x2 = (-b-w1*x1)/w2\n",
    "x_min = min(x[0])\n",
    "x_max = max(x[0])\n",
    "plt.plot(x1,x2, color='k')\n",
    "\n",
    "plt.savefig(\"problem 3_b_1\")\n",
    "plt.figure(1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# second plot code:\n",
    "plt.plot(num_iters[:j],J[:j])\n",
    "plt.savefig(\"problem 3_b_2\")\n",
    "plt.figure(2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EECS 598 Hw2 SVM Implementation\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class SVMLinearClassifier(object):\n",
    "  def __init__(self):\n",
    "    random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    self.W = None\n",
    "\n",
    "  def train(self, X_train, y_train, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    train_args = (self.loss, self.W, X_train, y_train, learning_rate, reg,\n",
    "                  num_iters, batch_size, verbose)\n",
    "    self.W, loss_history = train_linear_classifier(*train_args)\n",
    "    return loss_history\n",
    "\n",
    "  def predict(self, X):\n",
    "    return predict_linear_classifier(self.W, X)\n",
    "\n",
    "  @abstractmethod\n",
    "  def loss(self, W, X_batch, y_batch, reg):\n",
    "    \"\"\"\n",
    "    Compute the loss function and its derivative.\n",
    "    Subclasses will override this.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A PyTorch tensor of shape (D, C) containing (trained) weight of a model.\n",
    "    - X_batch: A PyTorch tensor of shape (N, D) containing a minibatch of N\n",
    "      data points; each point has dimension D.\n",
    "    - y_batch: A PyTorch tensor of shape (N,) containing labels for the minibatch.\n",
    "    - reg: (float) regularization strength.\n",
    "\n",
    "    Returns: A tuple containing:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to self.W; an tensor of the same shape as W\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def _loss(self, X_batch, y_batch, reg):\n",
    "    self.loss(self.W, X_batch, y_batch, reg)\n",
    "\n",
    "  def save(self, path):\n",
    "    torch.save({'W': self.W}, path)\n",
    "    print(\"Saved in {}\".format(path))\n",
    "\n",
    "  def load(self, path):\n",
    "    W_dict = torch.load(path, map_location='cpu')\n",
    "    self.W = W_dict['W']\n",
    "    print(\"load checkpoint file: {}\".format(path))\n",
    "\n",
    "\n",
    "\n",
    "def svm_loss_vectorized(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Structured SVM loss function, vectorized implementation. When you implment\n",
    "  the regularization over W, please DO NOT multiply the regularization term by\n",
    "  1/2 (no coefficient). The inputs and outputs are the same as svm_loss_naive.\n",
    "\n",
    "  Inputs:\n",
    "  - W: A PyTorch tensor of shape (D, C) containing weights.\n",
    "  - X: A PyTorch tensor of shape (N, D) containing a minibatch of data.\n",
    "  - y: A PyTorch tensor of shape (N,) containing training labels; y[i] = c means\n",
    "    that X[i] has label c, where 0 <= c < C.\n",
    "  - reg: (float) regularization strength\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss as torch scalar\n",
    "  - gradient of loss with respect to weights W; a tensor of same shape as W\n",
    "  \"\"\"\n",
    "  loss = 0.0\n",
    "  dW = torch.zeros_like(W) # initialize the gradient as zero\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO:                                                                     #\n",
    "  # Implement a vectorized version of the structured SVM loss, storing the    #\n",
    "  # result in loss.                                                           #\n",
    "  #############################################################################\n",
    "  # Replace \"pass\" statement with your code\n",
    "  scores = (X.matmul(W)).t() # CxN matrix\n",
    "  N = scores.shape[1]\n",
    "  temp_index = torch.arange(N)\n",
    "  correct_scores = scores[y, temp_index]\n",
    "  Margins = scores - correct_scores + torch.ones_like(scores)\n",
    "  Margins[Margins<0] = 0\n",
    "  loss = Margins.sum()/N - 1 # get the average loss and subtract 1 for the j=y_i case\n",
    "  loss += reg * torch.sum(W * W)\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO:                                                                     #\n",
    "  # Implement a vectorized version of the gradient for the structured SVM     #\n",
    "  # loss, storing the result in dW.                                           #\n",
    "  #                                                                           #\n",
    "  # Hint: Instead of computing the gradient from scratch, it may be easier    #\n",
    "  # to reuse some of the intermediate values that you used to compute the     #\n",
    "  # loss.                                                                     #\n",
    "  #############################################################################\n",
    "  # Replace \"pass\" statement with your code\n",
    "  Margins[Margins>0] = 1\n",
    "  Margins[y, temp_index] = 0 # Set the y[i] margins to 0\n",
    "  Margin_row = Margins.sum(dim=0) # Get the proper sum of positive margins where j != y_i\n",
    "  Margins[y, temp_index] = -Margin_row # Set the value of the y_i margin row in each column to negative of the sum of rows\n",
    "  # The above line simplifies the operation into a single matrix multiplication\n",
    "  dW = (Margins.matmul(X)).t()/N # Transpose to get the same shape as W\n",
    "  dW += 2*reg*W # Add reg. term\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "\n",
    "  return loss, dW\n",
    "\n",
    "def sample_batch(X, y, num_train, batch_size):\n",
    "  \"\"\"\n",
    "  Sample batch_size elements from the training data and their\n",
    "  corresponding labels to use in this round of gradient descent.\n",
    "  \"\"\"\n",
    "  X_batch = None\n",
    "  y_batch = None\n",
    "  #########################################################################\n",
    "  # TODO: Store the data in X_batch and their corresponding labels in     #\n",
    "  # y_batch; after sampling, X_batch should have shape (batch_size, dim)  #\n",
    "  # and y_batch should have shape (batch_size,)                           #\n",
    "  #                                                                       #\n",
    "  # Hint: Use torch.randint to generate indices.                          #\n",
    "  #########################################################################\n",
    "  # Replace \"pass\" statement with your code\n",
    "  i = torch.randint(0,num_train, [batch_size])\n",
    "  X_batch = X[i,:]\n",
    "  y_batch = y[i]\n",
    "  #########################################################################\n",
    "  #                       END OF YOUR CODE                                #\n",
    "  #########################################################################\n",
    "  return X_batch, y_batch\n",
    "\n",
    "\n",
    "def train_linear_classifier(loss_func, W, X, y, learning_rate=1e-3,\n",
    "                            reg=1e-5, num_iters=100, batch_size=200,\n",
    "                            verbose=False):\n",
    "  \"\"\"\n",
    "  Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "  Inputs:\n",
    "  - loss_func: loss function to use when training. It should take W, X, y\n",
    "    and reg as input, and output a tuple of (loss, dW)\n",
    "  - W: A PyTorch tensor of shape (D, C) giving the initial weights of the\n",
    "    classifier. If W is None then it will be initialized here.\n",
    "  - X: A PyTorch tensor of shape (N, D) containing training data; there are N\n",
    "    training samples each of dimension D.\n",
    "  - y: A PyTorch tensor of shape (N,) containing training labels; y[i] = c\n",
    "    means that X[i] has label 0 <= c < C for C classes.\n",
    "  - learning_rate: (float) learning rate for optimization.\n",
    "  - reg: (float) regularization strength.\n",
    "  - num_iters: (integer) number of steps to take when optimizing\n",
    "  - batch_size: (integer) number of training examples to use at each step.\n",
    "  - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "  Returns: A tuple of:\n",
    "  - W: The final value of the weight matrix and the end of optimization\n",
    "  - loss_history: A list of Python scalars giving the values of the loss at each\n",
    "    training iteration.\n",
    "  \"\"\"\n",
    "  # assume y takes values 0...K-1 where K is number of classes\n",
    "  num_train, dim = X.shape\n",
    "  if W is None:\n",
    "    # lazily initialize W\n",
    "    num_classes = torch.max(y) + 1\n",
    "    W = 0.000001 * torch.randn(dim, num_classes, device=X.device, dtype=X.dtype)\n",
    "  else:\n",
    "    num_classes = W.shape[1]\n",
    "\n",
    "  # Run stochastic gradient descent to optimize W\n",
    "  loss_history = []\n",
    "  for it in range(num_iters):\n",
    "    # TODO: implement sample_batch function\n",
    "    X_batch, y_batch = sample_batch(X, y, num_train, batch_size)\n",
    "\n",
    "    # evaluate loss and gradient\n",
    "    loss, grad = loss_func(W, X_batch, y_batch, reg)\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    # perform parameter update\n",
    "    #########################################################################\n",
    "    # TODO:                                                                 #\n",
    "    # Update the weights using the gradient and the learning rate.          #\n",
    "    #########################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    W -= learning_rate * grad\n",
    "    #########################################################################\n",
    "    #                       END OF YOUR CODE                                #\n",
    "    #########################################################################\n",
    "\n",
    "    if verbose and it % 100 == 0:\n",
    "      print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "  return W, loss_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
